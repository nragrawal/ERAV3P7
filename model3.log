model3.log

CUDA Available? True
Requirement already satisfied: torchsummary in /usr/local/lib/python3.10/dist-packages (1.5.1)
cuda
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1            [-1, 8, 26, 26]              72
              ReLU-2            [-1, 8, 26, 26]               0
            Conv2d-3           [-1, 16, 24, 24]           1,152
              ReLU-4           [-1, 16, 24, 24]               0
            Conv2d-5           [-1, 10, 24, 24]             160
         MaxPool2d-6           [-1, 10, 12, 12]               0
            Conv2d-7           [-1, 10, 10, 10]             900
              ReLU-8           [-1, 10, 10, 10]               0
            Conv2d-9             [-1, 16, 8, 8]           1,440
             ReLU-10             [-1, 16, 8, 8]               0
           Conv2d-11             [-1, 16, 6, 6]           2,304
             ReLU-12             [-1, 16, 6, 6]               0
           Conv2d-13             [-1, 10, 6, 6]           1,440
        AvgPool2d-14             [-1, 10, 1, 1]               0
================================================================
Total params: 7,468
Trainable params: 7,468
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.00
Forward/backward pass size (MB): 0.32
Params size (MB): 0.03
Estimated Total Size (MB): 0.35
----------------------------------------------------------------
EPOCH: 0
Loss=0.3064463138580322 Batch_id=468 Accuracy=33.76: 100%|██████████| 469/469 [00:17<00:00, 27.17it/s]
Test set: Average loss: 0.3727, Accuracy: 8926/10000 (89.26%)

EPOCH: 1
Loss=0.16233839094638824 Batch_id=468 Accuracy=93.72: 100%|██████████| 469/469 [00:21<00:00, 21.77it/s]
Test set: Average loss: 0.1106, Accuracy: 9676/10000 (96.76%)

EPOCH: 2
Loss=0.07457027584314346 Batch_id=468 Accuracy=96.13: 100%|██████████| 469/469 [00:17<00:00, 27.33it/s]
Test set: Average loss: 0.0755, Accuracy: 9778/10000 (97.78%)

EPOCH: 3
Loss=0.07709725946187973 Batch_id=468 Accuracy=97.04: 100%|██████████| 469/469 [00:19<00:00, 23.98it/s]
Test set: Average loss: 0.0765, Accuracy: 9768/10000 (97.68%)

EPOCH: 4
Loss=0.05258653685450554 Batch_id=468 Accuracy=97.51: 100%|██████████| 469/469 [00:17<00:00, 26.51it/s]
Test set: Average loss: 0.0589, Accuracy: 9826/10000 (98.26%)

EPOCH: 5
Loss=0.058135997503995895 Batch_id=468 Accuracy=97.71: 100%|██████████| 469/469 [00:18<00:00, 25.60it/s]
Test set: Average loss: 0.0508, Accuracy: 9840/10000 (98.40%)

EPOCH: 6
Loss=0.06626509130001068 Batch_id=468 Accuracy=98.05: 100%|██████████| 469/469 [00:24<00:00, 18.82it/s]
Test set: Average loss: 0.0470, Accuracy: 9854/10000 (98.54%)

EPOCH: 7
Loss=0.05355224013328552 Batch_id=468 Accuracy=98.19: 100%|██████████| 469/469 [00:20<00:00, 23.07it/s]
Test set: Average loss: 0.0490, Accuracy: 9839/10000 (98.39%)

EPOCH: 8
Loss=0.021391017362475395 Batch_id=468 Accuracy=98.36: 100%|██████████| 469/469 [00:18<00:00, 25.56it/s]
Test set: Average loss: 0.0551, Accuracy: 9828/10000 (98.28%)

EPOCH: 9
Loss=0.010606441646814346 Batch_id=468 Accuracy=98.48: 100%|██████████| 469/469 [00:17<00:00, 26.93it/s]
Test set: Average loss: 0.0404, Accuracy: 9876/10000 (98.76%)

EPOCH: 10
Loss=0.013129685074090958 Batch_id=468 Accuracy=98.53: 100%|██████████| 469/469 [00:17<00:00, 26.43it/s]
Test set: Average loss: 0.0412, Accuracy: 9874/10000 (98.74%)

EPOCH: 11
Loss=0.007769646123051643 Batch_id=468 Accuracy=98.62: 100%|██████████| 469/469 [00:16<00:00, 27.84it/s]
Test set: Average loss: 0.0503, Accuracy: 9852/10000 (98.52%)

EPOCH: 12
Loss=0.032810334116220474 Batch_id=468 Accuracy=98.72: 100%|██████████| 469/469 [00:17<00:00, 27.32it/s]
Test set: Average loss: 0.0460, Accuracy: 9847/10000 (98.47%)

EPOCH: 13
Loss=0.02418750524520874 Batch_id=468 Accuracy=98.72: 100%|██████████| 469/469 [00:17<00:00, 26.36it/s]
Test set: Average loss: 0.0513, Accuracy: 9848/10000 (98.48%)

EPOCH: 14
Loss=0.04364219680428505 Batch_id=468 Accuracy=98.72: 100%|██████████| 469/469 [00:17<00:00, 27.11it/s]
Test set: Average loss: 0.0470, Accuracy: 9853/10000 (98.53%)

EPOCH: 15
Loss=0.06732339411973953 Batch_id=468 Accuracy=98.89: 100%|██████████| 469/469 [00:17<00:00, 26.38it/s]
Test set: Average loss: 0.0410, Accuracy: 9874/10000 (98.74%)

EPOCH: 16
Loss=0.0059994664043188095 Batch_id=468 Accuracy=98.90: 100%|██████████| 469/469 [00:17<00:00, 26.57it/s]
Test set: Average loss: 0.0395, Accuracy: 9881/10000 (98.81%)

EPOCH: 17
Loss=0.03292039409279823 Batch_id=468 Accuracy=98.89: 100%|██████████| 469/469 [00:19<00:00, 23.57it/s]
Test set: Average loss: 0.0473, Accuracy: 9867/10000 (98.67%)

EPOCH: 18
Loss=0.02529236488044262 Batch_id=468 Accuracy=98.94: 100%|██████████| 469/469 [00:20<00:00, 23.16it/s]
Test set: Average loss: 0.0404, Accuracy: 9877/10000 (98.77%)

EPOCH: 19
Loss=0.01309287641197443 Batch_id=468 Accuracy=99.00: 100%|██████████| 469/469 [00:21<00:00, 21.74it/s]
Test set: Average loss: 0.0385, Accuracy: 9879/10000 (98.79%)


==================================================
| Epoch | Training Accuracy | Test Accuracy | Diff |
==================================================
|   0   |      33.76       |     89.26     | 55.50 |
|   1   |      93.72       |     96.76     | 3.05 |
|   2   |      96.13       |     97.78     | 1.65 |
|   3   |      97.04       |     97.68     | 0.64 |
|   4   |      97.51       |     98.26     | 0.75 |
|   5   |      97.71       |     98.40     | 0.69 |
|   6   |      98.05       |     98.54     | 0.49 |
|   7   |      98.19       |     98.39     | 0.20 |
|   8   |      98.36       |     98.28     | -0.08 |
|   9   |      98.48       |     98.76     | 0.28 |
|  10   |      98.53       |     98.74     | 0.21 |
|  11   |      98.62       |     98.52     | -0.10 |
|  12   |      98.72       |     98.47     | -0.25 |
|  13   |      98.72       |     98.48     | -0.24 |
|  14   |      98.72       |     98.53     | -0.19 |
|  15   |      98.89       |     98.74     | -0.15 |
|  16   |      98.90       |     98.81     | -0.09 |
|  17   |      98.89       |     98.67     | -0.22 |
|  18   |      98.94       |     98.77     | -0.17 |
|  19   |      99.00       |     98.79     | -0.21 |
==================================================
