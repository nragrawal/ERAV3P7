model1.log

CUDA Available? True
Requirement already satisfied: torchsummary in /usr/local/lib/python3.10/dist-packages (1.5.1)
cuda
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 16, 26, 26]             144
              ReLU-2           [-1, 16, 26, 26]               0
            Conv2d-3           [-1, 32, 24, 24]           4,608
              ReLU-4           [-1, 32, 24, 24]               0
            Conv2d-5           [-1, 64, 22, 22]          18,432
              ReLU-6           [-1, 64, 22, 22]               0
            Conv2d-7           [-1, 16, 22, 22]           1,024
         MaxPool2d-8           [-1, 16, 11, 11]               0
            Conv2d-9             [-1, 32, 9, 9]           4,608
             ReLU-10             [-1, 32, 9, 9]               0
           Conv2d-11             [-1, 64, 7, 7]          18,432
             ReLU-12             [-1, 64, 7, 7]               0
           Conv2d-13            [-1, 128, 5, 5]          73,728
             ReLU-14            [-1, 128, 5, 5]               0
           Conv2d-15             [-1, 10, 5, 5]          11,520
             ReLU-16             [-1, 10, 5, 5]               0
           Conv2d-17             [-1, 10, 1, 1]           2,500
================================================================
Total params: 134,996
Trainable params: 134,996
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.00
Forward/backward pass size (MB): 1.13
Params size (MB): 0.51
Estimated Total Size (MB): 1.65
----------------------------------------------------------------
EPOCH: 0
Loss=0.38356876373291016 Batch_id=468 Accuracy=30.63: 100%|██████████| 469/469 [00:18<00:00, 25.90it/s]
Test set: Average loss: 0.2492, Accuracy: 9257/10000 (92.57%)

EPOCH: 1
Loss=0.18412180244922638 Batch_id=468 Accuracy=94.94: 100%|██████████| 469/469 [00:18<00:00, 25.77it/s]
Test set: Average loss: 0.0844, Accuracy: 9736/10000 (97.36%)

EPOCH: 2
Loss=0.05623403191566467 Batch_id=468 Accuracy=97.36: 100%|██████████| 469/469 [00:17<00:00, 26.50it/s]
Test set: Average loss: 0.0623, Accuracy: 9803/10000 (98.03%)

EPOCH: 3
Loss=0.025586580857634544 Batch_id=468 Accuracy=98.09: 100%|██████████| 469/469 [00:17<00:00, 26.84it/s]
Test set: Average loss: 0.0511, Accuracy: 9837/10000 (98.37%)

EPOCH: 4
Loss=0.022775843739509583 Batch_id=468 Accuracy=98.40: 100%|██████████| 469/469 [00:17<00:00, 26.12it/s]
Test set: Average loss: 0.0510, Accuracy: 9842/10000 (98.42%)

EPOCH: 5
Loss=0.010226604528725147 Batch_id=468 Accuracy=98.71: 100%|██████████| 469/469 [00:18<00:00, 25.83it/s]
Test set: Average loss: 0.0472, Accuracy: 9865/10000 (98.65%)

EPOCH: 6
Loss=0.11511804908514023 Batch_id=468 Accuracy=98.89: 100%|██████████| 469/469 [00:17<00:00, 26.62it/s]
Test set: Average loss: 0.0404, Accuracy: 9880/10000 (98.80%)

EPOCH: 7
Loss=0.023741615936160088 Batch_id=468 Accuracy=99.02: 100%|██████████| 469/469 [00:18<00:00, 25.16it/s]
Test set: Average loss: 0.0437, Accuracy: 9880/10000 (98.80%)

EPOCH: 8
Loss=0.021491041406989098 Batch_id=468 Accuracy=99.14: 100%|██████████| 469/469 [00:17<00:00, 26.56it/s]
Test set: Average loss: 0.0413, Accuracy: 9877/10000 (98.77%)

EPOCH: 9
Loss=0.0070679583586752415 Batch_id=468 Accuracy=99.24: 100%|██████████| 469/469 [00:18<00:00, 25.84it/s]
Test set: Average loss: 0.0363, Accuracy: 9892/10000 (98.92%)

EPOCH: 10
Loss=0.02466430701315403 Batch_id=468 Accuracy=99.34: 100%|██████████| 469/469 [00:17<00:00, 26.70it/s]
Test set: Average loss: 0.0438, Accuracy: 9880/10000 (98.80%)

EPOCH: 11
Loss=0.0834989845752716 Batch_id=468 Accuracy=99.38: 100%|██████████| 469/469 [00:18<00:00, 25.81it/s]
Test set: Average loss: 0.0367, Accuracy: 9888/10000 (98.88%)

EPOCH: 12
Loss=0.0023744027130305767 Batch_id=468 Accuracy=99.47: 100%|██████████| 469/469 [00:23<00:00, 20.15it/s]
Test set: Average loss: 0.0428, Accuracy: 9879/10000 (98.79%)

EPOCH: 13
Loss=0.01744496449828148 Batch_id=468 Accuracy=99.51: 100%|██████████| 469/469 [00:18<00:00, 25.75it/s]
Test set: Average loss: 0.0445, Accuracy: 9866/10000 (98.66%)

EPOCH: 14
Loss=0.01966068707406521 Batch_id=468 Accuracy=99.56: 100%|██████████| 469/469 [00:18<00:00, 25.52it/s]
Test set: Average loss: 0.0423, Accuracy: 9880/10000 (98.80%)

EPOCH: 15
Loss=0.03893805667757988 Batch_id=468 Accuracy=99.57: 100%|██████████| 469/469 [00:17<00:00, 26.29it/s]
Test set: Average loss: 0.0383, Accuracy: 9893/10000 (98.93%)

EPOCH: 16
Loss=0.01088807824999094 Batch_id=468 Accuracy=99.67: 100%|██████████| 469/469 [00:17<00:00, 26.28it/s]
Test set: Average loss: 0.0469, Accuracy: 9892/10000 (98.92%)

EPOCH: 17
Loss=0.004390551242977381 Batch_id=468 Accuracy=99.65: 100%|██████████| 469/469 [00:18<00:00, 26.01it/s]
Test set: Average loss: 0.0432, Accuracy: 9891/10000 (98.91%)

EPOCH: 18
Loss=0.0035326890647411346 Batch_id=468 Accuracy=99.67: 100%|██████████| 469/469 [00:18<00:00, 25.87it/s]
Test set: Average loss: 0.0407, Accuracy: 9898/10000 (98.98%)

EPOCH: 19
Loss=0.00039872853085398674 Batch_id=468 Accuracy=99.71: 100%|██████████| 469/469 [00:17<00:00, 26.20it/s]
Test set: Average loss: 0.0479, Accuracy: 9891/10000 (98.91%)


==================================================
| Epoch | Training Accuracy | Test Accuracy | Diff |
==================================================
|   0   |      30.63       |     92.57     | 61.94 |
|   1   |      94.94       |     97.36     | 2.42 |
|   2   |      97.36       |     98.03     | 0.67 |
|   3   |      98.09       |     98.37     | 0.28 |
|   4   |      98.40       |     98.42     | 0.02 |
|   5   |      98.71       |     98.65     | -0.06 |
|   6   |      98.89       |     98.80     | -0.09 |
|   7   |      99.02       |     98.80     | -0.22 |
|   8   |      99.14       |     98.77     | -0.37 |
|   9   |      99.24       |     98.92     | -0.32 |
|  10   |      99.34       |     98.80     | -0.54 |
|  11   |      99.38       |     98.88     | -0.50 |
|  12   |      99.47       |     98.79     | -0.68 |
|  13   |      99.51       |     98.66     | -0.85 |
|  14   |      99.56       |     98.80     | -0.76 |
|  15   |      99.57       |     98.93     | -0.64 |
|  16   |      99.67       |     98.92     | -0.75 |
|  17   |      99.65       |     98.91     | -0.74 |
|  18   |      99.67       |     98.98     | -0.69 |
|  19   |      99.71       |     98.91     | -0.80 |
==================================================
