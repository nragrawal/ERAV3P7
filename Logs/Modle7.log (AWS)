Modle7.log (AWS)

CUDA Available? True
Requirement already satisfied: torchsummary in /opt/conda/envs/pytorch/lib/python3.11/site-packages (1.5.1)
cuda
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1            [-1, 8, 26, 26]              72
              ReLU-2            [-1, 8, 26, 26]               0
       BatchNorm2d-3            [-1, 8, 26, 26]              16
           Dropout-4            [-1, 8, 26, 26]               0
            Conv2d-5           [-1, 12, 24, 24]             864
              ReLU-6           [-1, 12, 24, 24]               0
       BatchNorm2d-7           [-1, 12, 24, 24]              24
           Dropout-8           [-1, 12, 24, 24]               0
            Conv2d-9           [-1, 10, 24, 24]             120
        MaxPool2d-10           [-1, 10, 12, 12]               0
           Conv2d-11           [-1, 10, 10, 10]             900
             ReLU-12           [-1, 10, 10, 10]               0
      BatchNorm2d-13           [-1, 10, 10, 10]              20
          Dropout-14           [-1, 10, 10, 10]               0
           Conv2d-15             [-1, 16, 8, 8]           1,440
             ReLU-16             [-1, 16, 8, 8]               0
      BatchNorm2d-17             [-1, 16, 8, 8]              32
          Dropout-18             [-1, 16, 8, 8]               0
           Conv2d-19             [-1, 16, 6, 6]           2,304
             ReLU-20             [-1, 16, 6, 6]               0
      BatchNorm2d-21             [-1, 16, 6, 6]              32
          Dropout-22             [-1, 16, 6, 6]               0
           Conv2d-23             [-1, 10, 6, 6]           1,440
        AvgPool2d-24             [-1, 10, 1, 1]               0
================================================================
Total params: 7,264
Trainable params: 7,264
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.00
Forward/backward pass size (MB): 0.51
Params size (MB): 0.03
Estimated Total Size (MB): 0.54
----------------------------------------------------------------
EPOCH: 0
Loss=0.03403529152274132 Batch_id=468 Accuracy=90.71: 100%|██████████| 469/469 [00:05<00:00, 91.02it/s] 

Test set: Average loss: 0.0703, Accuracy: 9783/10000 (97.83%)

EPOCH: 1
Loss=0.06745114922523499 Batch_id=468 Accuracy=98.02: 100%|██████████| 469/469 [00:05<00:00, 93.58it/s]  

Test set: Average loss: 0.0413, Accuracy: 9875/10000 (98.75%)

EPOCH: 2
Loss=0.059890855103731155 Batch_id=468 Accuracy=98.38: 100%|██████████| 469/469 [00:05<00:00, 89.21it/s]  

Test set: Average loss: 0.0348, Accuracy: 9888/10000 (98.88%)

EPOCH: 3
Loss=0.018943602219223976 Batch_id=468 Accuracy=98.55: 100%|██████████| 469/469 [00:05<00:00, 89.07it/s] 

Test set: Average loss: 0.0316, Accuracy: 9904/10000 (99.04%)

EPOCH: 4
Loss=0.019081834703683853 Batch_id=468 Accuracy=98.75: 100%|██████████| 469/469 [00:05<00:00, 88.88it/s] 

Test set: Average loss: 0.0265, Accuracy: 9921/10000 (99.21%)

EPOCH: 5
Loss=0.01167366188019514 Batch_id=468 Accuracy=98.91: 100%|██████████| 469/469 [00:05<00:00, 88.71it/s]  

Test set: Average loss: 0.0303, Accuracy: 9904/10000 (99.04%)

EPOCH: 6
Loss=0.008342118002474308 Batch_id=468 Accuracy=98.89: 100%|██████████| 469/469 [00:05<00:00, 90.26it/s] 

Test set: Average loss: 0.0222, Accuracy: 9937/10000 (99.37%)

EPOCH: 7
Loss=0.009315953589975834 Batch_id=468 Accuracy=98.99: 100%|██████████| 469/469 [00:04<00:00, 94.44it/s]  

Test set: Average loss: 0.0267, Accuracy: 9914/10000 (99.14%)

EPOCH: 8
Loss=0.004126036074012518 Batch_id=468 Accuracy=99.03: 100%|██████████| 469/469 [00:05<00:00, 92.54it/s]  

Test set: Average loss: 0.0241, Accuracy: 9924/10000 (99.24%)

EPOCH: 9
Loss=0.06420157849788666 Batch_id=468 Accuracy=99.08: 100%|██████████| 469/469 [00:05<00:00, 89.96it/s]   

Test set: Average loss: 0.0229, Accuracy: 9932/10000 (99.32%)

EPOCH: 10
Loss=0.014481988735496998 Batch_id=468 Accuracy=99.28: 100%|██████████| 469/469 [00:05<00:00, 89.18it/s] 

Test set: Average loss: 0.0171, Accuracy: 9945/10000 (99.45%)

EPOCH: 11
Loss=0.00475161662325263 Batch_id=468 Accuracy=99.37: 100%|██████████| 469/469 [00:05<00:00, 88.25it/s]  

Test set: Average loss: 0.0165, Accuracy: 9947/10000 (99.47%)

EPOCH: 12
Loss=0.008783583529293537 Batch_id=468 Accuracy=99.34: 100%|██████████| 469/469 [00:05<00:00, 91.41it/s]  

Test set: Average loss: 0.0162, Accuracy: 9952/10000 (99.52%)

EPOCH: 13
Loss=0.01520545408129692 Batch_id=468 Accuracy=99.44: 100%|██████████| 469/469 [00:05<00:00, 88.44it/s]  

Test set: Average loss: 0.0165, Accuracy: 9948/10000 (99.48%)

EPOCH: 14
Loss=0.0037630749866366386 Batch_id=468 Accuracy=99.39: 100%|██████████| 469/469 [00:05<00:00, 89.37it/s] 

Test set: Average loss: 0.0166, Accuracy: 9947/10000 (99.47%)

EPOCH: 15
Loss=0.032006848603487015 Batch_id=468 Accuracy=99.38: 100%|██████████| 469/469 [00:05<00:00, 90.88it/s] 

Test set: Average loss: 0.0164, Accuracy: 9946/10000 (99.46%)

EPOCH: 16
Loss=0.04549187421798706 Batch_id=468 Accuracy=99.43: 100%|██████████| 469/469 [00:05<00:00, 91.54it/s]  

Test set: Average loss: 0.0164, Accuracy: 9951/10000 (99.51%)

EPOCH: 17
Loss=0.002473758067935705 Batch_id=468 Accuracy=99.42: 100%|██████████| 469/469 [00:05<00:00, 87.13it/s] 

Test set: Average loss: 0.0165, Accuracy: 9949/10000 (99.49%)

EPOCH: 18
Loss=0.00688891438767314 Batch_id=468 Accuracy=99.44: 100%|██████████| 469/469 [00:05<00:00, 87.89it/s]   

Test set: Average loss: 0.0164, Accuracy: 9945/10000 (99.45%)

EPOCH: 19
Loss=0.010770868510007858 Batch_id=468 Accuracy=99.43: 100%|██████████| 469/469 [00:05<00:00, 86.02it/s] 

Test set: Average loss: 0.0166, Accuracy: 9950/10000 (99.50%)


==================================================
| Epoch | Training Accuracy | Test Accuracy | Diff |
==================================================
|   0   |      90.71       |     97.83     | 7.12 |
|   1   |      98.02       |     98.75     | 0.73 |
|   2   |      98.38       |     98.88     | 0.50 |
|   3   |      98.55       |     99.04     | 0.50 |
|   4   |      98.75       |     99.21     | 0.46 |
|   5   |      98.91       |     99.04     | 0.13 |
|   6   |      98.89       |     99.37     | 0.48 |
|   7   |      98.99       |     99.14     | 0.15 |
|   8   |      99.03       |     99.24     | 0.21 |
|   9   |      99.08       |     99.32     | 0.24 |
|  10   |      99.28       |     99.45     | 0.17 |
|  11   |      99.37       |     99.47     | 0.10 |
|  12   |      99.34       |     99.52     | 0.18 |
|  13   |      99.44       |     99.48     | 0.04 |
|  14   |      99.39       |     99.47     | 0.08 |
|  15   |      99.38       |     99.46     | 0.08 |
|  16   |      99.43       |     99.51     | 0.08 |
|  17   |      99.42       |     99.49     | 0.07 |
|  18   |      99.44       |     99.45     | 0.01 |
|  19   |      99.43       |     99.50     | 0.07 |
==================================================

