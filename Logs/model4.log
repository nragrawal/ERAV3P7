model4.log

CUDA Available? True
Requirement already satisfied: torchsummary in /usr/local/lib/python3.10/dist-packages (1.5.1)
cuda
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1            [-1, 8, 26, 26]              72
              ReLU-2            [-1, 8, 26, 26]               0
       BatchNorm2d-3            [-1, 8, 26, 26]              16
           Dropout-4            [-1, 8, 26, 26]               0
            Conv2d-5           [-1, 16, 24, 24]           1,152
              ReLU-6           [-1, 16, 24, 24]               0
       BatchNorm2d-7           [-1, 16, 24, 24]              32
           Dropout-8           [-1, 16, 24, 24]               0
            Conv2d-9           [-1, 10, 24, 24]             160
        MaxPool2d-10           [-1, 10, 12, 12]               0
           Conv2d-11           [-1, 10, 10, 10]             900
             ReLU-12           [-1, 10, 10, 10]               0
      BatchNorm2d-13           [-1, 10, 10, 10]              20
          Dropout-14           [-1, 10, 10, 10]               0
           Conv2d-15             [-1, 16, 8, 8]           1,440
             ReLU-16             [-1, 16, 8, 8]               0
      BatchNorm2d-17             [-1, 16, 8, 8]              32
          Dropout-18             [-1, 16, 8, 8]               0
           Conv2d-19             [-1, 16, 6, 6]           2,304
             ReLU-20             [-1, 16, 6, 6]               0
      BatchNorm2d-21             [-1, 16, 6, 6]              32
          Dropout-22             [-1, 16, 6, 6]               0
           Conv2d-23             [-1, 10, 6, 6]           1,440
        AvgPool2d-24             [-1, 10, 1, 1]               0
================================================================
Total params: 7,600
Trainable params: 7,600
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.00
Forward/backward pass size (MB): 0.58
Params size (MB): 0.03
Estimated Total Size (MB): 0.62
----------------------------------------------------------------
EPOCH: 0
Loss=0.10219207406044006 Batch_id=468 Accuracy=88.07: 100%|██████████| 469/469 [00:17<00:00, 26.85it/s]
Test set: Average loss: 0.0863, Accuracy: 9764/10000 (97.64%)

EPOCH: 1
Loss=0.16798186302185059 Batch_id=468 Accuracy=97.47: 100%|██████████| 469/469 [00:17<00:00, 26.40it/s]
Test set: Average loss: 0.0531, Accuracy: 9838/10000 (98.38%)

EPOCH: 2
Loss=0.05371474847197533 Batch_id=468 Accuracy=98.06: 100%|██████████| 469/469 [00:18<00:00, 25.82it/s]
Test set: Average loss: 0.0384, Accuracy: 9888/10000 (98.88%)

EPOCH: 3
Loss=0.018362348899245262 Batch_id=468 Accuracy=98.27: 100%|██████████| 469/469 [00:18<00:00, 25.08it/s]
Test set: Average loss: 0.0407, Accuracy: 9880/10000 (98.80%)

EPOCH: 4
Loss=0.01353445928543806 Batch_id=468 Accuracy=98.48: 100%|██████████| 469/469 [00:19<00:00, 24.53it/s]
Test set: Average loss: 0.0371, Accuracy: 9876/10000 (98.76%)

EPOCH: 5
Loss=0.028089134022593498 Batch_id=468 Accuracy=98.55: 100%|██████████| 469/469 [00:17<00:00, 27.40it/s]
Test set: Average loss: 0.0334, Accuracy: 9896/10000 (98.96%)

EPOCH: 6
Loss=0.06933767348527908 Batch_id=468 Accuracy=98.67: 100%|██████████| 469/469 [00:17<00:00, 26.28it/s]
Test set: Average loss: 0.0305, Accuracy: 9902/10000 (99.02%)

EPOCH: 7
Loss=0.02081095427274704 Batch_id=468 Accuracy=98.72: 100%|██████████| 469/469 [00:17<00:00, 27.09it/s]
Test set: Average loss: 0.0339, Accuracy: 9884/10000 (98.84%)

EPOCH: 8
Loss=0.04711965098977089 Batch_id=468 Accuracy=98.81: 100%|██████████| 469/469 [00:17<00:00, 27.00it/s]
Test set: Average loss: 0.0278, Accuracy: 9909/10000 (99.09%)

EPOCH: 9
Loss=0.009445042349398136 Batch_id=468 Accuracy=98.79: 100%|██████████| 469/469 [00:17<00:00, 26.97it/s]
Test set: Average loss: 0.0307, Accuracy: 9903/10000 (99.03%)

EPOCH: 10
Loss=0.017547564581036568 Batch_id=468 Accuracy=98.88: 100%|██████████| 469/469 [00:17<00:00, 26.97it/s]
Test set: Average loss: 0.0282, Accuracy: 9910/10000 (99.10%)

EPOCH: 11
Loss=0.010972883552312851 Batch_id=468 Accuracy=98.88: 100%|██████████| 469/469 [00:17<00:00, 26.89it/s]
Test set: Average loss: 0.0271, Accuracy: 9906/10000 (99.06%)

EPOCH: 12
Loss=0.003623451106250286 Batch_id=468 Accuracy=98.96: 100%|██████████| 469/469 [00:17<00:00, 26.52it/s]
Test set: Average loss: 0.0257, Accuracy: 9917/10000 (99.17%)

EPOCH: 13
Loss=0.12008097022771835 Batch_id=468 Accuracy=98.95: 100%|██████████| 469/469 [00:17<00:00, 26.09it/s]
Test set: Average loss: 0.0276, Accuracy: 9912/10000 (99.12%)

EPOCH: 14
Loss=0.020449388772249222 Batch_id=468 Accuracy=98.94: 100%|██████████| 469/469 [00:17<00:00, 26.82it/s]
Test set: Average loss: 0.0234, Accuracy: 9921/10000 (99.21%)

EPOCH: 15
Loss=0.06926899403333664 Batch_id=468 Accuracy=99.00: 100%|██████████| 469/469 [00:18<00:00, 25.84it/s]
Test set: Average loss: 0.0257, Accuracy: 9915/10000 (99.15%)

EPOCH: 16
Loss=0.0427573062479496 Batch_id=468 Accuracy=99.05: 100%|██████████| 469/469 [00:18<00:00, 25.78it/s]
Test set: Average loss: 0.0238, Accuracy: 9919/10000 (99.19%)

EPOCH: 17
Loss=0.04990970715880394 Batch_id=468 Accuracy=99.08: 100%|██████████| 469/469 [00:17<00:00, 26.92it/s]
Test set: Average loss: 0.0238, Accuracy: 9918/10000 (99.18%)

EPOCH: 18
Loss=0.01635214127600193 Batch_id=468 Accuracy=99.07: 100%|██████████| 469/469 [00:17<00:00, 26.50it/s]
Test set: Average loss: 0.0217, Accuracy: 9928/10000 (99.28%)

EPOCH: 19
Loss=0.04091264307498932 Batch_id=468 Accuracy=99.08: 100%|██████████| 469/469 [00:17<00:00, 26.55it/s]
Test set: Average loss: 0.0225, Accuracy: 9927/10000 (99.27%)


==================================================
| Epoch | Training Accuracy | Test Accuracy | Diff |
==================================================
|   0   |      88.07       |     97.64     | 9.57 |
|   1   |      97.47       |     98.38     | 0.91 |
|   2   |      98.06       |     98.88     | 0.82 |
|   3   |      98.27       |     98.80     | 0.53 |
|   4   |      98.48       |     98.76     | 0.28 |
|   5   |      98.55       |     98.96     | 0.41 |
|   6   |      98.67       |     99.02     | 0.35 |
|   7   |      98.72       |     98.84     | 0.12 |
|   8   |      98.81       |     99.09     | 0.28 |
|   9   |      98.79       |     99.03     | 0.24 |
|  10   |      98.88       |     99.10     | 0.22 |
|  11   |      98.88       |     99.06     | 0.18 |
|  12   |      98.96       |     99.17     | 0.21 |
|  13   |      98.95       |     99.12     | 0.17 |
|  14   |      98.94       |     99.21     | 0.27 |
|  15   |      99.00       |     99.15     | 0.15 |
|  16   |      99.05       |     99.19     | 0.14 |
|  17   |      99.08       |     99.18     | 0.10 |
|  18   |      99.07       |     99.28     | 0.21 |
|  19   |      99.08       |     99.27     | 0.19 |
==================================================
